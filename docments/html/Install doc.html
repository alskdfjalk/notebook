<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
<link rel="Stylesheet" type="text/css" href="style.css">
<title>Install doc</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
</head>
<body>


<p>
Hadoop下载地址 <a href="http://mirrors.hust.edu.cn/apache/hadoop/common/hadoop-2.6.2/hadoop-2.6.2-src.tar.gz">http://mirrors.hust.edu.cn/apache/hadoop/common/hadoop-2.6.2/hadoop-2.6.2-src.tar.gz</a> 
</p>

<p>
返回<a href="Hadoop.html">Hadoop</a>
</p>

<h1 id="toc_1">README</h1>
<pre>
	For the latest information about Hadoop, please visit our website at:
	
	   http://hadoop.apache.org/core/
	
	and our wiki, at:
	
	   http://wiki.apache.org/hadoop/
	
	This distribution includes cryptographic software.  The country in 
	which you currently reside may have restrictions on the import, 
	possession, use, and/or re-export to another country, of 
	encryption software.  BEFORE using any encryption software, please 
	check your country's laws, regulations and policies concerning the
	import, possession, or use, and re-export of encryption software, to 
	see if this is permitted.  See &lt;http://www.wassenaar.org/&gt; for more
	information.
	
	该应用(hadoop)包含了加密软件(模块、组件). 你所在的地区可能会限制
	该类软件的使用。在把非公开软件使用和／或重新发布给其他地址时，
	需要确保你所在的地区的法律允许。
	
</pre>
<p>
	授权详情请访问： <a href="http://www.wassenaar.org">http://www.wassenaar.org</a>
</p>
<pre>
	The U.S. Government Department of Commerce, Bureau of Industry and
	Security (BIS), has classified this software as Export Commodity 
	Control Number (ECCN) 5D002.C.1, which includes information security
	software using or performing cryptographic functions with asymmetric
	algorithms.  The form and manner of this Apache Software Foundation
	distribution makes it eligible for export under the License Exception
	ENC Technology 
	----------------------------------------------------------------------------------
	Building components separately
	各个组件的构建
	
	If you are building a submodule directory, all the hadoop dependencies this
	submodule has will be resolved as all other 3rd party dependencies. This is,
	from the Maven cache or from a Maven repository (if not available in the cache
	or the SNAPSHOT 'timed out').
	An alternative is to run 'mvn install -DskipTests' from Hadoop source top
	level once; and then work from the submodule. Keep in mind that SNAPSHOTs
	time out after a while, using the Maven '-nsu' will stop Maven from trying
	to update SNAPSHOTs from external repos.
	
	如果你要构建一个项目外的模块作为当前项目的子模块，Hadoop会负责从maven库中解决
	所有第三方依赖
	这里有一个替代方法是在Hadoop源码根目录执行一次"mvn install -DskipTests"，
	然后就会在自模块中运行.如果使用'-nsu'选项,将会禁止更新maven中其他repos的快照，
	否则，运行一段时间后会超时。
	
	----------------------------------------------------------------------------------
	Protocol Buffer compiler
	相当于一个消息格式的协议的编译器（解释器）
</pre>
<p>
	Protocol Buffer 介绍： <a href="http://www.cnblogs.com/stephen-liu74/archive/2013/01/02/2841485.html">http://www.cnblogs.com/stephen-liu74/archive/2013/01/02/2841485.html</a>
</p>
<pre>
	
	The version of Protocol Buffer compiler, protoc, must match the version of the
	protobuf JAR.
	该编译器版本必须和（Java）版本匹配
	
	If you have multiple versions of protoc in your system, you can set in your 
	build shell the HADOOP_PROTOC_PATH environment variable to point to the one you 
	want to use for the Hadoop build. If you don't define this environment variable,
	protoc is looked up in the PATH.
	如果有安装了多个protoc版本，可以通过配置HADOOP_PROTOC_PATH环境变量来指定要使用
	哪一个版本，如果没有指定，默认将使用PATH中的版本
	----------------------------------------------------------------------------------
	Importing projects to eclipse
	
	When you import the project to eclipse, install hadoop-maven-plugins at first.
	(Eclipse 是一个开放源代码的、基于Java的可扩展开发平台。相当于一个即成开发环境。
	就其本身而言，它只是一个框架和一组服务，用于通过插件组件构建开发环境。幸运的是，
	Eclipse 附带了一个标准的插件集，包括Java开发工具（Java Development Kit，JDK）)
	如果项目中使用了eclipse，需要安装 hadoop-maven-plugins, 安装方式：
	
	  $ cd hadoop-maven-plugins
	  $ mvn install
	
	Then, generate eclipse project files.
	然后生成eclipse的项目文件
	
	  $ mvn eclipse:eclipse -DskipTests
	
	At last, import to eclipse by specifying the root directory of the project via
	[File] &gt; [Import] &gt; [Existing Projects into Workspace].
	图形界面的操作，滚粗，直接无视。
	
	----------------------------------------------------------------------------------
	Building distributions:
	开始构建／安装
	
	Create binary distribution without native code and without documentation:
	不使用原生代码，并且不需要文档的情况下构建一个二进制的分发包
	
	  $ mvn package -Pdist -DskipTests -Dtar
	
	Create binary distribution with native code and with documentation:
	与上一条相反，包含文档以及原生代码
	
	  $ mvn package -Pdist,native,docs -DskipTests -Dtar
	
	Create source distribution:
	创建一个源码分发包
	
	  $ mvn package -Psrc -DskipTests
	
	Create source and binary distributions with native code and documentation:
	创建一个包含原生代码和文档的源码及二进制包
	
	  $ mvn package -Pdist,native,docs,src -DskipTests -Dtar
	
	Create a local staging version of the website (in /tmp/hadoop-site)
	创建一个本地环境的站点
	
	  $ mvn clean site; mvn site:stage -DstagingDirectory=/tmp/hadoop-site
	
	----------------------------------------------------------------------------------
	Installing Hadoop
	终于要开始安装了啊，你妹啊
	
	Look for these HTML files after you build the document by the above commands.
	卧槽，还需要去别处看，你妹啊
	
	  * Single Node Setup:
	    hadoop-project-dist/hadoop-common/SingleCluster.html
	
	  * Cluster Setup:
	    hadoop-project-dist/hadoop-common/ClusterSetup.html
	    
	    你妹妹啊，路径根本就没有这个文件好不好
	    笔者只找到了这个：hadoop-2.6.2/hadoop-common-project/hadoop-common/src/site/apt/SingleCluster.apt.vm
	    为了保证本文档结构的完整性，上述文档内容请看结尾。
	
	----------------------------------------------------------------------------------
	
	Handling out of memory errors in builds
	
	----------------------------------------------------------------------------------
	
	If the build process fails with an out of memory error, you should be able to fix
	it by increasing the memory used by maven -which can be done via the environment
	variable MAVEN_OPTS.
	
	Here is an example setting to allocate between 256 and 512 MB of heap space to
	Maven
	
	export MAVEN_OPTS="-Xms256m -Xmx512m"
	
	----------------------------------------------------------------------------------
	
	Building on OS/X
	
	----------------------------------------------------------------------------------
	
	A one-time manual step is required to enable building Hadoop OS X with Java 7
	every time the JDK is updated.
	see: https://issues.apache.org/jira/browse/HADOOP-9350
	
	$ sudo mkdir `/usr/libexec/java_home`/Classes
	$ sudo ln -s `/usr/libexec/java_home`/lib/tools.jar `/usr/libexec/java_home`/Classes/classes.jar
	
	----------------------------------------------------------------------------------
	
	Building on Windows
	
	----------------------------------------------------------------------------------
	Requirements:
	
	* Windows System
	* JDK 1.6+
	* Maven 3.0 or later
	* Findbugs 1.3.9 (if running findbugs)
	* ProtocolBuffer 2.5.0
	* CMake 2.6 or newer
	* Windows SDK or Visual Studio 2010 Professional
	* Unix command-line tools from GnuWin32 or Cygwin: sh, mkdir, rm, cp, tar, gzip
	* zlib headers (if building native code bindings for zlib)
	* Internet connection for first build (to fetch all Maven and Hadoop dependencies)
	
	If using Visual Studio, it must be Visual Studio 2010 Professional (not 2012).
	Do not use Visual Studio Express.  It does not support compiling for 64-bit,
	which is problematic if running a 64-bit system.  The Windows SDK is free to
	download here:
	
	http://www.microsoft.com/en-us/download/details.aspx?id=8279
	
	----------------------------------------------------------------------------------
	Building:
	
	Keep the source code tree in a short path to avoid running into problems related
	to Windows maximum path length limitation.  (For example, C:\hdc).
	
	Run builds from a Windows SDK Command Prompt.  (Start, All Programs,
	Microsoft Windows SDK v7.1, Windows SDK 7.1 Command Prompt.)
	
	JAVA_HOME must be set, and the path must not contain spaces.  If the full path
	would contain spaces, then use the Windows short path instead.
	
	You must set the Platform environment variable to either x64 or Win32 depending
	on whether you're running a 64-bit or 32-bit system.  Note that this is
	case-sensitive.  It must be "Platform", not "PLATFORM" or "platform".
	Environment variables on Windows are usually case-insensitive, but Maven treats
	them as case-sensitive.  Failure to set this environment variable correctly will
	cause msbuild to fail while building the native code in hadoop-common.
	
	set Platform=x64 (when building on a 64-bit system)
	set Platform=Win32 (when building on a 32-bit system)
	
	Several tests require that the user must have the Create Symbolic Links
	privilege.
	
	All Maven goals are the same as described above with the exception that
	native code is built by enabling the 'native-win' Maven profile. -Pnative-win 
	is enabled by default when building on Windows since the native components 
	are required (not optional) on Windows.
	
	If native code bindings for zlib are required, then the zlib headers must be
	deployed on the build machine.  Set the ZLIB_HOME environment variable to the
	directory containing the headers.
	
	set ZLIB_HOME=C:\zlib-1.2.7
	
	At runtime, zlib1.dll must be accessible on the PATH.  Hadoop has been tested
	with zlib 1.2.7, built using Visual Studio 2010 out of contrib\vstudio\vc10 in
	the zlib 1.2.7 source tree.
	
	http://www.zlib.net/
	
	----------------------------------------------------------------------------------
	Building distributions:
	
	 * Build distribution with native code    : mvn package [-Pdist][-Pdocs][-Psrc][-Dtar]
</pre>
<h2 id="toc_1.1">授权</h2>
<p>
 Licensed under the Apache License, Version 2.0 (the "License");
 you may not use this file except in compliance with the License.
 You may obtain a copy of the License at
</p>

<p>
   <a href="http://www.apache.org/licenses/LICENSE-2.0">http://www.apache.org/licenses/LICENSE-2.0</a>
</p>

<p>
 Unless required by applicable law or agreed to in writing, software
 distributed under the License is distributed on an "AS IS" BASIS,
 WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 See the License for the specific language governing permissions and
 limitations under the License. See accompanying LICENSE file.
</p>
  
<h2 id="toc_1.2">Hadoop MapReduce Next Generation - Setting up a Single Node Cluster.</h2>

<h3 id="toc_1.2.1">Purpose</h3>

<p>
  This document describes how to set up and configure a single-node Hadoop
  installation so that you can quickly perform simple operations using Hadoop
  MapReduce and the Hadoop Distributed File System (HDFS).
</p>

<h3 id="toc_1.2.2">Prerequisites</h3>

<h4 id="toc_1.2.2.1">Supported Platforms</h4>

<ol>
<li>
GNU/Linux is supported as a development and production platform.Hadoop has been demonstrated on GNU/Linux clusters with 2000 nodes.

<li>
Windows is also a supported platform but the followings steps are for Linux only. To set up Hadoop on Windows, see <a href="http://wiki.apache.org/hadoop/Hadoop2OnWindows">wiki page.</a>

</ol>

<h4 id="toc_1.2.2.2">Required Software</h4>

<ul>
<li>
Required software for Linux include

<ol>
<li>
Java must be installed. Recommended Java versions are described <a href="http://wiki.apache.org/hadoop/HadoopJavaVersions">HadoopJavaVersions</a>

<li>
ssh must be installed and sshd must be running to use the Hadoop scripts that manage remote Hadoop daemons.

</ol>
</ul>

<ul>
<li>
Installing Software
  If your cluster doesn't have the requisite software you will need to install it.
<pre>
	  For example on Ubuntu Linux:
	
		$ sudo apt-get install ssh
		$ sudo apt-get install rsync
</pre>

<li>
Download

<ul>
<li>
To get a Hadoop distribution, download a recent stable release from one of
  the <a href="http://www.apache.org/dyn/closer.cgi/hadoop/common/">Apache Download Mirrors</a>

</ul>
</ul>

<ul>
<li>
Prepare to Start the Hadoop Cluster<br/>

<ul>
<li>
Unpack the downloaded Hadoop distribution. In the distribution, edit
  the file &lt;&lt;&lt;etc/hadoop/hadoop-env.sh&gt;&gt;&gt; to define some parameters as
  follows:
<pre>
		# set to the root of your Java installation
		export JAVA_HOME=/usr/java/latest

		# Assuming your installation directory is /usr/local/hadoop
		export HADOOP_PREFIX=/usr/local/hadoop
</pre>

<li>
Try the following command:
<pre>
		$ bin/hadoop
</pre>

<li>
This will display the usage documentation for the hadoop script.<br/>

</ul>
</ul>
 
<ul>
<li>
Now you are ready to start your Hadoop cluster in one of the three supported
  modes:

<ul>
<li>
{{{Standalone Operation}Local (Standalone) Mode}}

<li>
{{{Pseudo-Distributed Operation}Pseudo-Distributed Mode}}

<li>
{{{Fully-Distributed Operation}Fully-Distributed Mode}}

</ul>
</ul>

<h4 id="toc_1.2.2.3">Standalone Operation</h4>
<ul>
<li>
By default, Hadoop is configured to run in a non-distributed mode, as a
  single Java process. This is useful for debugging.

</ul>

<p>
  The following example copies the unpacked conf directory to use as input
  and then finds and displays every match of the given regular expression.
  Output is written to the given output directory.
</p>

<hr />
<p>
  $ mkdir input
  $ cp etc/hadoop/*.xml input
  \( bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-\){project.version}.jar grep input output 'dfs[a-z.]+'
  $ cat output/*
<hr />
</p>

<ul>
<li>
Pseudo-Distributed Operation

</ul>

<p>
  Hadoop can also be run on a single-node in a pseudo-distributed mode where
  each Hadoop daemon runs in a separate Java process.
</p>

<p>
** Configuration
</p>

<p>
  Use the following:
</p>

<p>
  etc/hadoop/core-site.xml:
</p>

<p>
+---+
&lt;configuration&gt;
</p>
<blockquote>
&lt;property&gt;
&lt;name&gt;fs.defaultFS&lt;/name&gt;
&lt;value&gt;hdfs://localhost:9000&lt;/value&gt;
&lt;/property&gt;
</blockquote>
<p>
&lt;/configuration&gt;
+---+
</p>

<p>
  etc/hadoop/hdfs-site.xml:
</p>

<p>
+---+
&lt;configuration&gt;
</p>
<blockquote>
&lt;property&gt;
&lt;name&gt;dfs.replication&lt;/name&gt;
&lt;value&gt;1&lt;/value&gt;
&lt;/property&gt;
</blockquote>
<p>
&lt;/configuration&gt;
+---+
</p>

<p>
** Setup passphraseless ssh
</p>

<p>
  Now check that you can ssh to the localhost without a passphrase:
</p>

<hr />
<p>
  $ ssh localhost
<hr />
</p>

<p>
  If you cannot ssh to localhost without a passphrase, execute the
  following commands:
</p>

<hr />
<p>
  $ ssh-keygen -t dsa -P '' -f ~/.ssh/id_dsa
  $ cat ~/.ssh/id_dsa.pub &gt;&gt; ~/.ssh/authorized_keys
<hr />
</p>

<p>
** Execution
</p>

<p>
  The following instructions are to run a MapReduce job locally.
  If you want to execute a job on YARN, see <img src="YARN on Single Node" />.
</p>

<p>
  <a href="1.html">1</a> Format the filesystem:
</p>

<hr />
<p>
  $ bin/hdfs namenode -format
<hr />
</p>

<p>
  <a href="2.html">2</a> Start NameNode daemon and DataNode daemon:
</p>

<hr />
<p>
  $ sbin/start-dfs.sh
<hr />
</p>
<blockquote>
The hadoop daemon log output is written to the &lt;&lt;&lt;${HADOOP_LOG_DIR}&gt;&gt;&gt;
directory (defaults to &lt;&lt;&lt;${HADOOP_HOME}/logs&gt;&gt;&gt;).
</blockquote>

<p>
  <a href="3.html">3</a> Browse the web interface for the NameNode; by default it is
</p>
<blockquote>
available at:
</blockquote>

<ul>
<li>
NameNode - &lt;&lt;&lt;<a href="http://localhost:50070/&gt;&gt;&gt;">http://localhost:50070/&gt;&gt;&gt;</a>

</ul>

<p>
  <a href="4.html">4</a> Make the HDFS directories required to execute MapReduce jobs:
</p>

<hr />
<p>
  $ bin/hdfs dfs -mkdir /user
  $ bin/hdfs dfs -mkdir /user/&lt;username&gt;
<hr />
</p>

<p>
  <a href="5.html">5</a> Copy the input files into the distributed filesystem:
</p>

<hr />
<p>
  $ bin/hdfs dfs -put etc/hadoop input
<hr />
</p>

<p>
  <a href="6.html">6</a> Run some of the examples provided:
</p>

<hr />
<p>
  \( bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-\){project.version}.jar grep input output 'dfs[a-z.]+'
<hr />
</p>

<p>
  <a href="7.html">7</a> Examine the output files:
</p>
<blockquote>
Copy the output files from the distributed filesystem to the local
filesystem and examine them:
</blockquote>

<hr />
<p>
  $ bin/hdfs dfs -get output output
  $ cat output/*
<hr />
</p>
<blockquote>
or
</blockquote>
<blockquote>
View the output files on the distributed filesystem:
</blockquote>

<hr />
<p>
  $ bin/hdfs dfs -cat output/*
<hr />
</p>

<p>
  <a href="8.html">8</a> When you're done, stop the daemons with:
</p>

<hr />
<p>
  $ sbin/stop-dfs.sh
<hr />
</p>

<p>
** YARN on Single Node
</p>

<p>
  You can run a MapReduce job on YARN in a pseudo-distributed mode by setting
  a few parameters and running ResourceManager daemon and NodeManager daemon
  in addition.
</p>

<p>
  The following instructions assume that 1. ~ 4. steps of
</p>
<pre Execution}the above instructions}} are already executed.>

  [[1]] Configure parameters as follows:

        etc/hadoop/mapred-site.xml:

+---+
&lt;configuration&gt;
    &lt;property&gt;
        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;
        &lt;value&gt;yarn&lt;/value&gt;
    &lt;/property&gt;
&lt;/configuration&gt;
+---+

        etc/hadoop/yarn-site.xml:

+---+
&lt;configuration&gt;
    &lt;property&gt;
        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;
        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;
    &lt;/property&gt;
&lt;/configuration&gt;
+---+

  [[2]] Start ResourceManager daemon and NodeManager daemon:

----
  $ sbin/start-yarn.sh
----

  [[3]] Browse the web interface for the ResourceManager; by default it is
        available at:

        * ResourceManager - &lt;&lt;&lt;http://localhost:8088/&gt;&gt;&gt;

  [[4]] Run a MapReduce job.

  [[5]] When you're done, stop the daemons with:

----
  $ sbin/stop-yarn.sh
----

* Fully-Distributed Operation

  For information on setting up fully-distributed, non-trivial clusters
  see {{{./ClusterSetup.html}Cluster Setup}}.
</pre>

</body>
</html>
